What is the difference betwenn lossless and lossy compression?

Definition of Lossy Compression

The Lossy compression method eliminates some amount of data that is not noticeable.
This technique does not allow a file to restore in its original form but significantly reduces the size.
The lossy compression technique is beneficial if the quality of the data is not your priority.
It slightly degrades the quality of the file or data but is convenient when one wants to send or store the data.
This type of data compression is used for organic data like audio signals and images.

Definition of Lossless Compression

The Lossless compression method is capable of reconstituting the original form of the data.
The quality of the data is not compromised.
This technique allows a file to restore its original form.
Lossless compression can be applied to any file format can improve the performance of the compression ratio.

When can we get away with lossy compression?

As the definition above states lossy compression eliminates the data that is not noticeable.
This is said because for the most part we use lossy compression for images, videos and audio files.
Most of the information is still there and you would not notice a few milliseconds missing from the video or a couple of pixels missing from the image.
Basically everything without text. Unless you are into guessing missing chars from words I guess it is fine.

What is entropy?

???

How are Huffman trees constructed?

To begin generating the Huffman tree, each character gets a weight equal to the number of times it
occurs in the file. For example, in the "happy hip hop" example, the character 'p' has weight 4,
'h' has weight 3, the space has weight 2, and the other characters have weight 1. Our first task is to
calculate these weights, which we can do with a simple pass through the file to get the frequency
counts. For each character, we create an unattached tree node containing the character value and its
corresponding weight. You can think of each node as a tree with just one entry. The idea is to
combine all these separate trees into an optimal tree by wiring them together from the bottom upwards.
The general approach is as follows:

1. Create a collection of singleton trees, one for each character, with weight equal to the character
frequency.
2. From the collection, pick out the two trees with the smallest weights and remove them.
Combine them into a new tree whose root has a weight equal to the sum of the weights of the
two trees and with the two trees as its left and right subtrees.
3. Add the new combined tree back into the collection.
4. Repeat steps 2 and 3 until there is only one tree left.
5. The remaining node is the root of the optimal encoding tree. 

How can we get back the uncompressed data from the Huffman tree?

There is one last issue we have not discussed yet. Suppose that I want to compress a message and send
it to you. Using Huffman coding, I can convert the message (plus the pseudo-EOF) into a string of bits
and send it to you. However, you cannot decompress the message, because you don't have the
encoding tree that I used to send the message.
There are many ways to resolve this. We could agree on an encoding tree in advance, but this only
works if we already know the distribution of the letters in advance. This might be true if we were
always compressing normal English text, but in general is not possible to do.
A second option is to prefix the bit sequence with a header
containing enough information to reconstruct the Huffman encoding tree. There are many options you
have for reading and writing the encoding table. You could store the table at the head of the file in a
long, human-readable string format using the ASCII characters '0' and '1', one character entry per
each line, like this:
h = 01
a = 000
p = 10
y = 1111

Reading this back in would allow you to recreate the tree path by path. You could have a line for every
character in the ASCII set; characters that are unused would have an empty bit pattern. Or you could
conserve space by only listing those characters that appear in the encoding. In such a case you must
record a number that tells how many entries are in the table or put some sort of sentinel or marker at the
end so you know when you have processed them all.
As an alternative to storing sequences of ASCII '0' and '1' characters for the bit patterns, you could
store just the character frequency counts and rebuild the tree again from those counts in order to
decompress. Again we might include the counts for all characters (including those that are zero) or
optimize to only record those that are non-zero. Here is how we might encode the non-zero character
counts for the "happy hip hop" string (the 7 at the front says there are 7 entries to follow—6
alphabetic characters, and the space character):
6 h3 a1 p4 y1 2 i1 o1


